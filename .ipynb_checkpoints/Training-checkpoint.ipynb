{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb7d1b80-d3ec-4cc6-b514-6ce8428b8a6c",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55aec402-1fd2-498c-a9f1-5121f8348c1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/marta/jku/activity_fair\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "274157a4-5a83-427b-bda0-00a8c31a81c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "harmful_list = [0.00, 0.01, 0.05, 0.10]\n",
    "DATASET = 'ml-100k'\n",
    "# DATASET = 'amazon_digital_music'\n",
    "# DATASET = 'lastfm'\n",
    "MODEL_LIST = ['BPR', 'ItemKNN', 'MultiVAE']\n",
    "\n",
    "MODEL = 'MultiVAE'\n",
    "\n",
    "data_path_dict = {\n",
    "    f'{DATASET}_harm{str(int(100*harmful)).zfill(2)}':\n",
    "    '/home/marta/jku/activity_fair/datasets/filtered_datasets/' for harmful in harmful_list\n",
    "}\n",
    "\n",
    "config_dict = {\n",
    "    'BPR': 'bpr_config',\n",
    "    'ItemKNN': 'iknn_config',\n",
    "    'MultiVAE': 'vae_config',\n",
    "}\n",
    "\n",
    "params_dict = {\n",
    "    'BPR': 'bpr_params',\n",
    "    'ItemKNN': 'iknn_params',\n",
    "    'MultiVAE': 'vae_params',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50f69cac-ede2-4dd2-8f62-1e53db4e440b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ml-100k_harm00': '/home/marta/jku/activity_fair/datasets/filtered_datasets/',\n",
       " 'ml-100k_harm01': '/home/marta/jku/activity_fair/datasets/filtered_datasets/',\n",
       " 'ml-100k_harm05': '/home/marta/jku/activity_fair/datasets/filtered_datasets/',\n",
       " 'ml-100k_harm10': '/home/marta/jku/activity_fair/datasets/filtered_datasets/'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabe25eb-12d9-4749-8776-7c574b5f08f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running parameters:                                                             \n",
      "{'anneal_cap': 1.0, 'latent_dimension': 1000, 'learning_rate': 0.001, 'mlp_hidden_size': '[1000]', 'total_anneal_steps': 5000.0}\n",
      "  0%|                                    | 0/16 [00:00<?, ?trial/s, best loss=?]15 Feb 15:31    INFO  \u001B[1;35mSaving filtered dataset into \u001B[0m[/home/marta/jku/activity_fair/saved/ml-100k_harm00/ml-100k_harm00-dataset.pth]\u001B[0m\n",
      "\n",
      "==KF                                                                            \n",
      "5                                                                               \n",
      "phases: 3\t datasets: 3                                                          \n",
      "  0%|                                    | 0/16 [00:00<?, ?trial/s, best loss=?]15 Feb 15:31    INFO  \u001B[1;35mSaving split dataloaders into\u001B[0m: [/home/marta/jku/activity_fair/saved/ml-100k_harm00/ml-100k_harm00-for-MultiVAE-dataloader.pth]\u001B[0m\n",
      "\n",
      "15 Feb 15:31    INFO  \u001B[1;35m[Training]: \u001B[0m\u001B[1;36mtrain_batch_size\u001B[0m = \u001B[1;33m[1024]\u001B[0m\u001B[1;36m train_neg_sample_args\u001B[0m: \u001B[1;33m[{'distribution': 'uniform', 'sample_num': 1, 'alpha': 1.0, 'dynamic': False, 'candidate_num': 0}]\u001B[0m\n",
      "\n",
      "15 Feb 15:31    INFO  \u001B[1;35m[Evaluation]: \u001B[0m\u001B[1;36meval_batch_size\u001B[0m = \u001B[1;33m[1024]\u001B[0m\u001B[1;36m eval_args\u001B[0m: \u001B[1;33m[{'split': {'KF': [0.2, 0.2, 0.2, 0.2, 0.2]}, 'fold': 0, 'group_by': 'none', 'order': 'RO', 'mode': 'full'}]\u001B[0m\n",
      "\n",
      "\u001B[33m15 Feb 15:31    WARNING  Max value of user's history interaction records has reached 21.015873015873016% of the total.\u001B[0m\n",
      "\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mmmosc\u001B[0m. Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n",
      "\n",
      "^C\n",
      "Problem at:                                                                     \n",
      "/home/marta/jku/activity_fair/recbole/utils/wandblogger.py                      \n",
      "38                                                                              \n",
      "setup                                                                           \n",
      "  0%|                                    | 0/16 [00:04<?, ?trial/s, best loss=?]\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/marta/jku/activity_fair//run_hyper.py\", line 127, in <module>\n",
      "    hyperopt_tune(args)\n",
      "  File \"/home/marta/jku/activity_fair//run_hyper.py\", line 39, in hyperopt_tune\n",
      "    hp.run()\n",
      "  File \"/home/marta/jku/activity_fair/recbole/trainer/hyper_tuning.py\", line 413, in run\n",
      "    fmin(\n",
      "  File \"/home/marta/miniconda3/envs/recbole/lib/python3.8/site-packages/hyperopt/fmin.py\", line 553, in fmin\n",
      "    rval.exhaust()\n",
      "  File \"/home/marta/miniconda3/envs/recbole/lib/python3.8/site-packages/hyperopt/fmin.py\", line 356, in exhaust\n",
      "    self.run(self.max_evals - n_done, block_until_done=self.asynchronous)\n",
      "  File \"/home/marta/miniconda3/envs/recbole/lib/python3.8/site-packages/hyperopt/fmin.py\", line 292, in run\n",
      "    self.serial_evaluate()\n",
      "  File \"/home/marta/miniconda3/envs/recbole/lib/python3.8/site-packages/hyperopt/fmin.py\", line 170, in serial_evaluate\n",
      "    result = self.domain.evaluate(spec, ctrl)\n",
      "  File \"/home/marta/miniconda3/envs/recbole/lib/python3.8/site-packages/hyperopt/base.py\", line 907, in evaluate\n",
      "    rval = self.fn(pyll_rval)\n",
      "  File \"/home/marta/jku/activity_fair/recbole/trainer/hyper_tuning.py\", line 348, in trial\n",
      "    result_dict = self.objective_function(config_dict, self.fixed_config_file_list)\n",
      "  File \"/home/marta/jku/activity_fair/recbole/quick_start/quick_start.py\", line 147, in objective_function\n",
      "    trainer = get_trainer(config[\"MODEL_TYPE\"], config[\"model\"])(config, model)\n",
      "  File \"/home/marta/jku/activity_fair/recbole/trainer/trainer.py\", line 116, in __init__\n",
      "    self.wandblogger = WandbLogger(config)\n",
      "  File \"/home/marta/jku/activity_fair/recbole/utils/wandblogger.py\", line 22, in __init__\n",
      "    self.setup()\n",
      "  File \"/home/marta/jku/activity_fair/recbole/utils/wandblogger.py\", line 38, in setup\n",
      "    self._wandb.init(project=self.config.wandb_project, config=self.config)\n",
      "  File \"/home/marta/miniconda3/envs/recbole/lib/python3.8/site-packages/wandb/sdk/wandb_init.py\", line 1100, in init\n",
      "    raise e\n",
      "  File \"/home/marta/miniconda3/envs/recbole/lib/python3.8/site-packages/wandb/sdk/wandb_init.py\", line 1078, in init\n",
      "    run = wi.init()\n",
      "  File \"/home/marta/miniconda3/envs/recbole/lib/python3.8/site-packages/wandb/sdk/wandb_init.py\", line 739, in init\n",
      "    _ = backend.interface.communicate_run_start(run_obj)\n",
      "  File \"/home/marta/miniconda3/envs/recbole/lib/python3.8/site-packages/wandb/sdk/interface/interface.py\", line 235, in communicate_run_start\n",
      "    result = self._communicate_run_start(run_start)\n",
      "  File \"/home/marta/miniconda3/envs/recbole/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py\", line 484, in _communicate_run_start\n",
      "    result = self._communicate(rec)\n",
      "  File \"/home/marta/miniconda3/envs/recbole/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py\", line 255, in _communicate\n",
      "    return self._communicate_async(rec, local=local).get(timeout=timeout)\n",
      "  File \"/home/marta/miniconda3/envs/recbole/lib/python3.8/site-packages/wandb/sdk/interface/router.py\", line 36, in get\n",
      "    is_set = self._object_ready.wait(timeout)\n",
      "  File \"/home/marta/miniconda3/envs/recbole/lib/python3.8/threading.py\", line 558, in wait\n",
      "    signaled = self._cond.wait(timeout)\n",
      "  File \"/home/marta/miniconda3/envs/recbole/lib/python3.8/threading.py\", line 306, in wait\n",
      "    gotit = waiter.acquire(True, timeout)\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/marta/jku/activity_fair//run_hyper.py\", line 127, in <module>\n",
      "    hyperopt_tune(args)\n",
      "  File \"/home/marta/jku/activity_fair//run_hyper.py\", line 39, in hyperopt_tune\n",
      "    hp.run()\n",
      "  File \"/home/marta/jku/activity_fair/recbole/trainer/hyper_tuning.py\", line 413, in run\n",
      "    fmin(\n",
      "  File \"/home/marta/miniconda3/envs/recbole/lib/python3.8/site-packages/hyperopt/fmin.py\", line 553, in fmin\n",
      "    rval.exhaust()\n",
      "  File \"/home/marta/miniconda3/envs/recbole/lib/python3.8/site-packages/hyperopt/fmin.py\", line 356, in exhaust\n",
      "    self.run(self.max_evals - n_done, block_until_done=self.asynchronous)\n",
      "  File \"/home/marta/miniconda3/envs/recbole/lib/python3.8/site-packages/hyperopt/fmin.py\", line 292, in run\n",
      "    self.serial_evaluate()\n",
      "  File \"/home/marta/miniconda3/envs/recbole/lib/python3.8/site-packages/hyperopt/fmin.py\", line 170, in serial_evaluate\n",
      "    result = self.domain.evaluate(spec, ctrl)\n",
      "  File \"/home/marta/miniconda3/envs/recbole/lib/python3.8/site-packages/hyperopt/base.py\", line 907, in evaluate\n",
      "    rval = self.fn(pyll_rval)\n",
      "  File \"/home/marta/jku/activity_fair/recbole/trainer/hyper_tuning.py\", line 348, in trial\n",
      "    result_dict = self.objective_function(config_dict, self.fixed_config_file_list)\n",
      "  File \"/home/marta/jku/activity_fair/recbole/quick_start/quick_start.py\", line 147, in objective_function\n",
      "    trainer = get_trainer(config[\"MODEL_TYPE\"], config[\"model\"])(config, model)\n",
      "  File \"/home/marta/jku/activity_fair/recbole/trainer/trainer.py\", line 116, in __init__\n",
      "    self.wandblogger = WandbLogger(config)\n",
      "  File \"/home/marta/jku/activity_fair/recbole/utils/wandblogger.py\", line 22, in __init__\n",
      "    self.setup()\n",
      "  File \"/home/marta/jku/activity_fair/recbole/utils/wandblogger.py\", line 38, in setup\n",
      "    self._wandb.init(project=self.config.wandb_project, config=self.config)\n",
      "  File \"/home/marta/miniconda3/envs/recbole/lib/python3.8/site-packages/wandb/sdk/wandb_init.py\", line 1100, in init\n",
      "    raise e\n",
      "  File \"/home/marta/miniconda3/envs/recbole/lib/python3.8/site-packages/wandb/sdk/wandb_init.py\", line 1078, in init\n",
      "    run = wi.init()\n",
      "  File \"/home/marta/miniconda3/envs/recbole/lib/python3.8/site-packages/wandb/sdk/wandb_init.py\", line 739, in init\n",
      "    _ = backend.interface.communicate_run_start(run_obj)\n",
      "  File \"/home/marta/miniconda3/envs/recbole/lib/python3.8/site-packages/wandb/sdk/interface/interface.py\", line 235, in communicate_run_start\n",
      "    result = self._communicate_run_start(run_start)\n",
      "  File \"/home/marta/miniconda3/envs/recbole/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py\", line 484, in _communicate_run_start\n",
      "    result = self._communicate(rec)\n",
      "  File \"/home/marta/miniconda3/envs/recbole/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py\", line 255, in _communicate\n",
      "    return self._communicate_async(rec, local=local).get(timeout=timeout)\n",
      "  File \"/home/marta/miniconda3/envs/recbole/lib/python3.8/site-packages/wandb/sdk/interface/router.py\", line 36, in get\n",
      "    is_set = self._object_ready.wait(timeout)\n",
      "  File \"/home/marta/miniconda3/envs/recbole/lib/python3.8/threading.py\", line 558, in wait\n",
      "    signaled = self._cond.wait(timeout)\n",
      "  File \"/home/marta/miniconda3/envs/recbole/lib/python3.8/threading.py\", line 306, in wait\n",
      "    gotit = waiter.acquire(True, timeout)\n",
      "KeyboardInterrupt\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Waiting for W&B process to finish... \u001B[31m(failed 255).\u001B[0m Press Control-C to abort syncing.\n",
      "running parameters:                                                             \n",
      "{'anneal_cap': 1.0, 'latent_dimension': 1000, 'learning_rate': 0.001, 'mlp_hidden_size': '[1000]', 'total_anneal_steps': 100000.0}\n",
      "  0%|                                    | 0/16 [00:00<?, ?trial/s, best loss=?]15 Feb 15:31    INFO  \u001B[1;35mSaving filtered dataset into \u001B[0m[/home/marta/jku/activity_fair/saved/ml-100k_harm01/ml-100k_harm01-dataset.pth]\u001B[0m\n",
      "\n",
      "==KF                                                                            \n",
      "5                                                                               \n",
      "phases: 3\t datasets: 3                                                          \n",
      "  0%|                                    | 0/16 [00:00<?, ?trial/s, best loss=?]15 Feb 15:31    INFO  \u001B[1;35mSaving split dataloaders into\u001B[0m: [/home/marta/jku/activity_fair/saved/ml-100k_harm01/ml-100k_harm01-for-MultiVAE-dataloader.pth]\u001B[0m\n",
      "\n",
      "15 Feb 15:31    INFO  \u001B[1;35m[Training]: \u001B[0m\u001B[1;36mtrain_batch_size\u001B[0m = \u001B[1;33m[1024]\u001B[0m\u001B[1;36m train_neg_sample_args\u001B[0m: \u001B[1;33m[{'distribution': 'uniform', 'sample_num': 1, 'alpha': 1.0, 'dynamic': False, 'candidate_num': 0}]\u001B[0m\n",
      "\n",
      "15 Feb 15:31    INFO  \u001B[1;35m[Evaluation]: \u001B[0m\u001B[1;36meval_batch_size\u001B[0m = \u001B[1;33m[1024]\u001B[0m\u001B[1;36m eval_args\u001B[0m: \u001B[1;33m[{'split': {'KF': [0.2, 0.2, 0.2, 0.2, 0.2]}, 'fold': 0, 'group_by': 'none', 'order': 'RO', 'mode': 'full'}]\u001B[0m\n",
      "\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mmmosc\u001B[0m. Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n",
      "\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: wandb version 0.13.10 is available!  To upgrade, please run:\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m:  $ pip install wandb --upgrade\n",
      "\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Tracking run with wandb version 0.13.5\n",
      "\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Run data is saved locally in \u001B[35m\u001B[1m/home/marta/jku/activity_fair/wandb/run-20230215_153149-2a8k66vr\u001B[0m\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Run \u001B[1m`wandb offline`\u001B[0m to turn off syncing.\n",
      "\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Syncing run \u001B[33msteadfast-flower-46\u001B[0m\n",
      "\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m:  View project at \u001B[34m\u001B[4mhttps://wandb.ai/mmosc/recbole\u001B[0m\n",
      "\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m:  View run at \u001B[34m\u001B[4mhttps://wandb.ai/mmosc/recbole/runs/2a8k66vr\u001B[0m\n",
      "\n",
      "15 Feb 15:33    INFO  Loading model structure and parameters from /home/marta/jku/activity_fair/saved/ml-100k_harm01/MultiVAE-Feb-15-2023_15-31-53.pth\u001B[0m\n",
      "\n",
      "\u001B[33m15 Feb 15:33    WARNING  Session not detected. You should not be calling `report` outside `tuner.fit()` or while using the class API. \u001B[0m\n",
      "\n",
      "\u001B[33m15 Feb 15:33    WARNING    File \"/home/marta/jku/activity_fair//run_hyper.py\", line 127, in <module>\n",
      "    hyperopt_tune(args)\n",
      "  File \"/home/marta/jku/activity_fair//run_hyper.py\", line 39, in hyperopt_tune\n",
      "    hp.run()\n",
      "  File \"/home/marta/jku/activity_fair/recbole/trainer/hyper_tuning.py\", line 413, in run\n",
      "    fmin(\n",
      "  File \"/home/marta/miniconda3/envs/recbole/lib/python3.8/site-packages/hyperopt/fmin.py\", line 553, in fmin\n",
      "    rval.exhaust()\n",
      "  File \"/home/marta/miniconda3/envs/recbole/lib/python3.8/site-packages/hyperopt/fmin.py\", line 356, in exhaust\n",
      "    self.run(self.max_evals - n_done, block_until_done=self.asynchronous)\n",
      "  File \"/home/marta/miniconda3/envs/recbole/lib/python3.8/site-packages/hyperopt/fmin.py\", line 292, in run\n",
      "    self.serial_evaluate()\n",
      "  File \"/home/marta/miniconda3/envs/recbole/lib/python3.8/site-packages/hyperopt/fmin.py\", line 170, in serial_evaluate\n",
      "    result = self.domain.evaluate(spec, ctrl)\n",
      "  File \"/home/marta/miniconda3/envs/recbole/lib/python3.8/site-packages/hyperopt/base.py\", line 907, in evaluate\n",
      "    rval = self.fn(pyll_rval)\n",
      "  File \"/home/marta/jku/activity_fair/recbole/trainer/hyper_tuning.py\", line 348, in trial\n",
      "    result_dict = self.objective_function(config_dict, self.fixed_config_file_list)\n",
      "  File \"/home/marta/jku/activity_fair/recbole/quick_start/quick_start.py\", line 153, in objective_function\n",
      "    tune.report(**test_result)\n",
      "\u001B[0m\n",
      "\n",
      "current best valid score: 0.2731                                                \n",
      "current best valid result:                                                      \n",
      "OrderedDict([('recall@10', 0.196), ('mrr@10', 0.4833), ('ndcg@10', 0.2731), ('hit@10', 0.8591), ('precision@10', 0.2203)])\n",
      "current test result:                                                            \n",
      "OrderedDict([('recall@10', 0.2192), ('mrr@10', 0.5807), ('ndcg@10', 0.3477), ('hit@10', 0.8778), ('precision@10', 0.2792)])\n",
      "running parameters:                                                             \n",
      "{'anneal_cap': 1.0, 'latent_dimension': 1000, 'learning_rate': 0.0001, 'mlp_hidden_size': '[]', 'total_anneal_steps': 100000.0}\n",
      "  6%|█▎                  | 1/16 [01:45<26:26, 105.74s/trial, best loss: -0.2731]15 Feb 15:33    INFO  \u001B[1;35mSaving filtered dataset into \u001B[0m[/home/marta/jku/activity_fair/saved/ml-100k_harm01/ml-100k_harm01-dataset.pth]\u001B[0m\n",
      "\n",
      "phases: 3\t datasets: 3                                                          \n",
      "  6%|█▎                  | 1/16 [01:45<26:26, 105.74s/trial, best loss: -0.2731]15 Feb 15:33    INFO  \u001B[1;35mSaving split dataloaders into\u001B[0m: [/home/marta/jku/activity_fair/saved/ml-100k_harm01/ml-100k_harm01-for-MultiVAE-dataloader.pth]\u001B[0m\n",
      "\n",
      "15 Feb 15:33    INFO  \u001B[1;35m[Training]: \u001B[0m\u001B[1;36mtrain_batch_size\u001B[0m = \u001B[1;33m[1024]\u001B[0m\u001B[1;36m train_neg_sample_args\u001B[0m: \u001B[1;33m[{'distribution': 'uniform', 'sample_num': 1, 'alpha': 1.0, 'dynamic': False, 'candidate_num': 0}]\u001B[0m\n",
      "\n",
      "15 Feb 15:33    INFO  \u001B[1;35m[Evaluation]: \u001B[0m\u001B[1;36meval_batch_size\u001B[0m = \u001B[1;33m[1024]\u001B[0m\u001B[1;36m eval_args\u001B[0m: \u001B[1;33m[{'split': {'RS': [0.6, 0.2, 0.2]}, 'fold': 0, 'group_by': 'none', 'order': 'RO', 'mode': 'full'}]\u001B[0m\n",
      "\n",
      "15 Feb 15:36    INFO  Loading model structure and parameters from /home/marta/jku/activity_fair/saved/ml-100k_harm01/MultiVAE-Feb-15-2023_15-33-33.pth\u001B[0m\n",
      "\n",
      "running parameters:                                                             \n",
      "{'anneal_cap': 1.0, 'latent_dimension': 1000, 'learning_rate': 0.0001, 'mlp_hidden_size': '[1000]', 'total_anneal_steps': 5000.0}\n",
      " 12%|██▌                 | 2/16 [04:41<34:18, 147.00s/trial, best loss: -0.2731]15 Feb 15:36    INFO  \u001B[1;35mSaving filtered dataset into \u001B[0m[/home/marta/jku/activity_fair/saved/ml-100k_harm01/ml-100k_harm01-dataset.pth]\u001B[0m\n",
      "\n",
      "phases: 3\t datasets: 3                                                          \n",
      " 12%|██▌                 | 2/16 [04:41<34:18, 147.00s/trial, best loss: -0.2731]15 Feb 15:36    INFO  \u001B[1;35mSaving split dataloaders into\u001B[0m: [/home/marta/jku/activity_fair/saved/ml-100k_harm01/ml-100k_harm01-for-MultiVAE-dataloader.pth]\u001B[0m\n",
      "\n",
      "15 Feb 15:36    INFO  \u001B[1;35m[Training]: \u001B[0m\u001B[1;36mtrain_batch_size\u001B[0m = \u001B[1;33m[1024]\u001B[0m\u001B[1;36m train_neg_sample_args\u001B[0m: \u001B[1;33m[{'distribution': 'uniform', 'sample_num': 1, 'alpha': 1.0, 'dynamic': False, 'candidate_num': 0}]\u001B[0m\n",
      "\n",
      "15 Feb 15:36    INFO  \u001B[1;35m[Evaluation]: \u001B[0m\u001B[1;36meval_batch_size\u001B[0m = \u001B[1;33m[1024]\u001B[0m\u001B[1;36m eval_args\u001B[0m: \u001B[1;33m[{'split': {'RS': [0.6, 0.2, 0.2]}, 'fold': 0, 'group_by': 'none', 'order': 'RO', 'mode': 'full'}]\u001B[0m\n",
      "\n",
      "^C\n",
      " 12%|██▌                 | 2/16 [04:58<34:51, 149.37s/trial, best loss: -0.2731]\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/marta/jku/activity_fair//run_hyper.py\", line 127, in <module>\n",
      "    hyperopt_tune(args)\n",
      "  File \"/home/marta/jku/activity_fair//run_hyper.py\", line 39, in hyperopt_tune\n",
      "    hp.run()\n",
      "  File \"/home/marta/jku/activity_fair/recbole/trainer/hyper_tuning.py\", line 413, in run\n",
      "    fmin(\n",
      "  File \"/home/marta/miniconda3/envs/recbole/lib/python3.8/site-packages/hyperopt/fmin.py\", line 553, in fmin\n",
      "    rval.exhaust()\n",
      "  File \"/home/marta/miniconda3/envs/recbole/lib/python3.8/site-packages/hyperopt/fmin.py\", line 356, in exhaust\n",
      "    self.run(self.max_evals - n_done, block_until_done=self.asynchronous)\n",
      "  File \"/home/marta/miniconda3/envs/recbole/lib/python3.8/site-packages/hyperopt/fmin.py\", line 292, in run\n",
      "    self.serial_evaluate()\n",
      "  File \"/home/marta/miniconda3/envs/recbole/lib/python3.8/site-packages/hyperopt/fmin.py\", line 170, in serial_evaluate\n",
      "    result = self.domain.evaluate(spec, ctrl)\n",
      "  File \"/home/marta/miniconda3/envs/recbole/lib/python3.8/site-packages/hyperopt/base.py\", line 907, in evaluate\n",
      "    rval = self.fn(pyll_rval)\n",
      "  File \"/home/marta/jku/activity_fair/recbole/trainer/hyper_tuning.py\", line 348, in trial\n",
      "    result_dict = self.objective_function(config_dict, self.fixed_config_file_list)\n",
      "  File \"/home/marta/jku/activity_fair/recbole/quick_start/quick_start.py\", line 148, in objective_function\n",
      "    best_valid_score, best_valid_result = trainer.fit(\n",
      "  File \"/home/marta/jku/activity_fair/recbole/trainer/trainer.py\", line 462, in fit\n",
      "    valid_score, valid_result = self._valid_epoch(\n",
      "  File \"/home/marta/jku/activity_fair/recbole/trainer/trainer.py\", line 281, in _valid_epoch\n",
      "    valid_result = self.evaluate(\n",
      "  File \"/home/marta/miniconda3/envs/recbole/lib/python3.8/site-packages/torch/autograd/grad_mode.py\", line 28, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/marta/jku/activity_fair/recbole/trainer/trainer.py\", line 613, in evaluate\n",
      "    interaction, scores, positive_u, positive_i = eval_func(batched_data)\n",
      "  File \"/home/marta/jku/activity_fair/recbole/trainer/trainer.py\", line 522, in _full_sort_batch_eval\n",
      "    scores = self.model.full_sort_predict(interaction.to(self.device))\n",
      "  File \"/home/marta/jku/activity_fair/recbole/model/general_recommender/multivae.py\", line 152, in full_sort_predict\n",
      "    scores, _, _ = self.forward(rating_matrix)\n",
      "  File \"/home/marta/jku/activity_fair/recbole/model/general_recommender/multivae.py\", line 108, in forward\n",
      "    z = self.decoder(z)\n",
      "  File \"/home/marta/miniconda3/envs/recbole/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/marta/miniconda3/envs/recbole/lib/python3.8/site-packages/torch/nn/modules/container.py\", line 141, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/marta/miniconda3/envs/recbole/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/marta/miniconda3/envs/recbole/lib/python3.8/site-packages/torch/nn/modules/activation.py\", line 349, in forward\n",
      "    return torch.tanh(input)\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/marta/jku/activity_fair//run_hyper.py\", line 127, in <module>\n",
      "    hyperopt_tune(args)\n",
      "  File \"/home/marta/jku/activity_fair//run_hyper.py\", line 39, in hyperopt_tune\n",
      "    hp.run()\n",
      "  File \"/home/marta/jku/activity_fair/recbole/trainer/hyper_tuning.py\", line 413, in run\n",
      "    fmin(\n",
      "  File \"/home/marta/miniconda3/envs/recbole/lib/python3.8/site-packages/hyperopt/fmin.py\", line 553, in fmin\n",
      "    rval.exhaust()\n",
      "  File \"/home/marta/miniconda3/envs/recbole/lib/python3.8/site-packages/hyperopt/fmin.py\", line 356, in exhaust\n",
      "    self.run(self.max_evals - n_done, block_until_done=self.asynchronous)\n",
      "  File \"/home/marta/miniconda3/envs/recbole/lib/python3.8/site-packages/hyperopt/fmin.py\", line 292, in run\n",
      "    self.serial_evaluate()\n",
      "  File \"/home/marta/miniconda3/envs/recbole/lib/python3.8/site-packages/hyperopt/fmin.py\", line 170, in serial_evaluate\n",
      "    result = self.domain.evaluate(spec, ctrl)\n",
      "  File \"/home/marta/miniconda3/envs/recbole/lib/python3.8/site-packages/hyperopt/base.py\", line 907, in evaluate\n",
      "    rval = self.fn(pyll_rval)\n",
      "  File \"/home/marta/jku/activity_fair/recbole/trainer/hyper_tuning.py\", line 348, in trial\n",
      "    result_dict = self.objective_function(config_dict, self.fixed_config_file_list)\n",
      "  File \"/home/marta/jku/activity_fair/recbole/quick_start/quick_start.py\", line 148, in objective_function\n",
      "    best_valid_score, best_valid_result = trainer.fit(\n",
      "  File \"/home/marta/jku/activity_fair/recbole/trainer/trainer.py\", line 462, in fit\n",
      "    valid_score, valid_result = self._valid_epoch(\n",
      "  File \"/home/marta/jku/activity_fair/recbole/trainer/trainer.py\", line 281, in _valid_epoch\n",
      "    valid_result = self.evaluate(\n",
      "  File \"/home/marta/miniconda3/envs/recbole/lib/python3.8/site-packages/torch/autograd/grad_mode.py\", line 28, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/marta/jku/activity_fair/recbole/trainer/trainer.py\", line 613, in evaluate\n",
      "    interaction, scores, positive_u, positive_i = eval_func(batched_data)\n",
      "  File \"/home/marta/jku/activity_fair/recbole/trainer/trainer.py\", line 522, in _full_sort_batch_eval\n",
      "    scores = self.model.full_sort_predict(interaction.to(self.device))\n",
      "  File \"/home/marta/jku/activity_fair/recbole/model/general_recommender/multivae.py\", line 152, in full_sort_predict\n",
      "    scores, _, _ = self.forward(rating_matrix)\n",
      "  File \"/home/marta/jku/activity_fair/recbole/model/general_recommender/multivae.py\", line 108, in forward\n",
      "    z = self.decoder(z)\n",
      "  File \"/home/marta/miniconda3/envs/recbole/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/marta/miniconda3/envs/recbole/lib/python3.8/site-packages/torch/nn/modules/container.py\", line 141, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/marta/miniconda3/envs/recbole/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/marta/miniconda3/envs/recbole/lib/python3.8/site-packages/torch/nn/modules/activation.py\", line 349, in forward\n",
      "    return torch.tanh(input)\n",
      "KeyboardInterrupt\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Waiting for W&B process to finish... \u001B[31m(failed 255).\u001B[0m Press Control-C to abort syncing.\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/marta/jku/activity_fair//run_hyper.py\", line 15, in <module>\n",
      "    from recbole.trainer import HyperTuning\n",
      "  File \"/home/marta/jku/activity_fair/recbole/trainer/__init__.py\", line 1, in <module>\n",
      "    from recbole.trainer.hyper_tuning import HyperTuning\n",
      "  File \"/home/marta/jku/activity_fair/recbole/trainer/hyper_tuning.py\", line 21, in <module>\n",
      "    from recbole.utils.utils import dict2str\n",
      "  File \"/home/marta/jku/activity_fair/recbole/utils/__init__.py\", line 1, in <module>\n",
      "    from recbole.utils.logger import init_logger, set_color\n",
      "  File \"/home/marta/jku/activity_fair/recbole/utils/logger.py\", line 26, in <module>\n",
      "    from recbole.utils.utils import get_local_time, ensure_dir\n",
      "  File \"/home/marta/jku/activity_fair/recbole/utils/utils.py\", line 22, in <module>\n",
      "    import torch\n",
      "  File \"/home/marta/miniconda3/envs/recbole/lib/python3.8/site-packages/torch/__init__.py\", line 721, in <module>\n",
      "    import torch.utils.data\n",
      "  File \"/home/marta/miniconda3/envs/recbole/lib/python3.8/site-packages/torch/utils/data/__init__.py\", line 38, in <module>\n",
      "    from torch.utils.data.dataloader_experimental import DataLoader2\n",
      "  File \"/home/marta/miniconda3/envs/recbole/lib/python3.8/site-packages/torch/utils/data/dataloader_experimental.py\", line 11, in <module>\n",
      "    from torch.utils.data.datapipes.iter import IterableWrapper\n",
      "  File \"/home/marta/miniconda3/envs/recbole/lib/python3.8/site-packages/torch/utils/data/datapipes/__init__.py\", line 1, in <module>\n",
      "    from . import iter\n",
      "  File \"/home/marta/miniconda3/envs/recbole/lib/python3.8/site-packages/torch/utils/data/datapipes/iter/__init__.py\", line 37, in <module>\n",
      "    from torch.utils.data.datapipes.iter.selecting import (\n",
      "  File \"/home/marta/miniconda3/envs/recbole/lib/python3.8/site-packages/torch/utils/data/datapipes/iter/selecting.py\", line 7, in <module>\n",
      "    import pandas  # type: ignore[import]\n",
      "  File \"/home/marta/miniconda3/envs/recbole/lib/python3.8/site-packages/pandas/__init__.py\", line 22, in <module>\n",
      "    from pandas.compat import is_numpy_dev as _is_numpy_dev\n",
      "  File \"/home/marta/miniconda3/envs/recbole/lib/python3.8/site-packages/pandas/compat/__init__.py\", line 20, in <module>\n",
      "    from pandas.compat.pyarrow import (\n",
      "  File \"/home/marta/miniconda3/envs/recbole/lib/python3.8/site-packages/pandas/compat/pyarrow.py\", line 6, in <module>\n",
      "    import pyarrow as pa\n",
      "  File \"/home/marta/miniconda3/envs/recbole/lib/python3.8/site-packages/pyarrow/__init__.py\", line 65, in <module>\n",
      "    import pyarrow.lib as _lib\n",
      "  File \"<frozen importlib._bootstrap>\", line 389, in parent\n",
      "KeyboardInterrupt\n",
      "running parameters:                                                             \n",
      "{'anneal_cap': 1.0, 'latent_dimension': 1000, 'learning_rate': 0.001, 'mlp_hidden_size': '[1000]', 'total_anneal_steps': 5000.0}\n",
      "  0%|                                    | 0/48 [00:00<?, ?trial/s, best loss=?]15 Feb 15:36    INFO  \u001B[1;35mSaving filtered dataset into \u001B[0m[/home/marta/jku/activity_fair/saved/ml-100k_harm10/ml-100k_harm10-dataset.pth]\u001B[0m\n",
      "\n",
      "phases: 3\t datasets: 3                                                          \n",
      "  0%|                                    | 0/48 [00:00<?, ?trial/s, best loss=?]15 Feb 15:36    INFO  \u001B[1;35mSaving split dataloaders into\u001B[0m: [/home/marta/jku/activity_fair/saved/ml-100k_harm10/ml-100k_harm10-for-MultiVAE-dataloader.pth]\u001B[0m\n",
      "\n",
      "15 Feb 15:36    INFO  \u001B[1;35m[Training]: \u001B[0m\u001B[1;36mtrain_batch_size\u001B[0m = \u001B[1;33m[1024]\u001B[0m\u001B[1;36m train_neg_sample_args\u001B[0m: \u001B[1;33m[{'distribution': 'uniform', 'sample_num': 1, 'alpha': 1.0, 'dynamic': False, 'candidate_num': 0}]\u001B[0m\n",
      "\n",
      "15 Feb 15:36    INFO  \u001B[1;35m[Evaluation]: \u001B[0m\u001B[1;36meval_batch_size\u001B[0m = \u001B[1;33m[1024]\u001B[0m\u001B[1;36m eval_args\u001B[0m: \u001B[1;33m[{'split': {'RS': [0.6, 0.2, 0.2]}, 'fold': 0, 'group_by': 'none', 'order': 'RO', 'mode': 'full'}]\u001B[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "BASE_FOLDER = '/home/marta/jku/activity_fair/'\n",
    "\n",
    "CONFIG_FILE = config_dict[MODEL]\n",
    "PARAMS_FILE = params_dict[MODEL]\n",
    "\n",
    "for data_version, data_path in data_path_dict.items():\n",
    "    command = f\"python {BASE_FOLDER}/run_hyper.py \\\n",
    "    --model={MODEL} \\\n",
    "    --data_path={data_path} \\\n",
    "    --dataset={data_version} \\\n",
    "    --config_files={BASE_FOLDER}/config/{CONFIG_FILE}.yaml \\\n",
    "    --params_file={BASE_FOLDER}/config/{PARAMS_FILE}.yaml \\\n",
    "    --checkpoint_dir={BASE_FOLDER}saved/{data_version}\\\n",
    "    --tool=Hyperopt\"\n",
    "    !eval {command}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5c2ee9-5cd8-4d8c-9a18-b15c9e7ca8b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recbole",
   "language": "python",
   "name": "recbole"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
